{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import wespeaker.models.resnet  # Import the ResNet model from wespeaker.models module\n",
    "\n",
    "# Step 3: Load pre-trained model configuration\n",
    "with open(\"voxceleb_resnet152_LM/voxceleb_resnet152_LM.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Step 4: Load pre-trained model\n",
    "model = wespeaker.models.resnet.ResNet152(**config[\"model_args\"])\n",
    "\n",
    "# Load only compatible keys from the pre-trained model\n",
    "pretrained_dict = torch.load(\"voxceleb_resnet152_LM/voxceleb_resnet152_LM.pt\")\n",
    "model_dict = model.state_dict()\n",
    "\n",
    "# Filter out unnecessary keys\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "\n",
    "# Update model weights\n",
    "model.load_state_dict(pretrained_dict, strict=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root_dir, target_length=32000, transform=None):\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.label_to_index = {}\n",
    "        self.index_to_label = {}\n",
    "        self.target_length = target_length\n",
    "        self.transform = transform\n",
    "        subfolders = ['custom_noised', 'gauss', 'multiplied_amplitude', 'subsampled']\n",
    "        \n",
    "        for folder in subfolders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.endswith('.wav'):\n",
    "                    full_path = os.path.join(folder_path, filename)\n",
    "                    label = self.extract_label_from_filename(filename)\n",
    "                    if label not in self.label_to_index:\n",
    "                        self.label_to_index[label] = len(self.label_to_index)\n",
    "                        self.index_to_label[self.label_to_index[label]] = label\n",
    "                    self.files.append(full_path)\n",
    "                    self.labels.append(self.label_to_index[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.files[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Ensure waveform is mono\n",
    "        if waveform.ndim == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        \n",
    "        # Pad or truncate waveform to the target length\n",
    "        if waveform.shape[1] > self.target_length:\n",
    "            waveform = waveform[:, :self.target_length]\n",
    "        elif waveform.shape[1] < self.target_length:\n",
    "            padding_size = self.target_length - waveform.shape[1]\n",
    "            padding = torch.zeros((1, padding_size))\n",
    "            waveform = torch.cat((waveform, padding), 1)\n",
    "\n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform, sample_rate)\n",
    "        \n",
    "        return waveform, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_label_from_filename(filename):\n",
    "        parts = filename.split('_')\n",
    "        return ' '.join(parts[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_transform(waveform, sample_rate, target_sample_rate=16000):\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    waveforms, labels = zip(*batch)\n",
    "    max_length = max([waveform.shape[1] for waveform in waveforms])\n",
    "    waveforms_padded = torch.stack([\n",
    "        torch.cat([waveform, torch.zeros(1, max_length - waveform.shape[1])], dim=1) \n",
    "        if waveform.shape[1] < max_length else waveform for waveform in waveforms\n",
    "    ])\n",
    "    labels = torch.tensor(labels)\n",
    "    return waveforms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AudioDataset(root_dir='data_noise_train', transform=audio_transform)\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(32000, 100),  # Adjust according to your model's input size\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, len(dataset.label_to_index))  # Output size is number of unique labels\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Assuming you can modify the model at least by adding layers\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, original_model, output_features):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.original_model = original_model\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, output_features))  # Adjust the target output size\n",
    "        self.classifier = nn.Linear(output_features, len(dataset.label_to_index))  # Adjust based on your number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.adaptive_pool(x)  # Resize feature to match the expected input size of the classifier\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Example of wrapping an existing model\n",
    "# Let's assume `model` is your pre-loaded model that you can't alter directly\n",
    "model = CustomModel(model, 100)  # '100' should be adjusted based on your needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training Loss: 4.8339\n",
      "Epoch 2: Training Loss: 4.8339\n",
      "Epoch 3: Training Loss: 4.8339\n",
      "Epoch 4: Training Loss: 4.8340\n",
      "Epoch 5: Training Loss: 4.8339\n",
      "Epoch 6: Training Loss: 4.8340\n",
      "Epoch 7: Training Loss: 4.8338\n",
      "Epoch 8: Training Loss: 4.8339\n",
      "Epoch 9: Training Loss: 4.8338\n",
      "Epoch 10: Training Loss: 4.8339\n",
      "Epoch 11: Training Loss: 4.8339\n",
      "Epoch 12: Training Loss: 4.8339\n",
      "Epoch 13: Training Loss: 4.8339\n",
      "Epoch 14: Training Loss: 4.8339\n",
      "Epoch 15: Training Loss: 4.8339\n",
      "Epoch 16: Training Loss: 4.8339\n",
      "Epoch 17: Training Loss: 4.8339\n",
      "Epoch 18: Training Loss: 4.8338\n",
      "Epoch 19: Training Loss: 4.8338\n",
      "Epoch 20: Training Loss: 4.8339\n",
      "Epoch 21: Training Loss: 4.8339\n",
      "Epoch 22: Training Loss: 4.8339\n",
      "Epoch 23: Training Loss: 4.8338\n",
      "Epoch 24: Training Loss: 4.8338\n",
      "Epoch 25: Training Loss: 4.8339\n",
      "Epoch 26: Training Loss: 4.8338\n",
      "Epoch 27: Training Loss: 4.8339\n",
      "Epoch 28: Training Loss: 4.8339\n",
      "Epoch 29: Training Loss: 4.8338\n",
      "Epoch 30: Training Loss: 4.8339\n",
      "Epoch 31: Training Loss: 4.8339\n",
      "Epoch 32: Training Loss: 4.8340\n",
      "Epoch 33: Training Loss: 4.8340\n",
      "Epoch 34: Training Loss: 4.8339\n",
      "Epoch 35: Training Loss: 4.8339\n",
      "Epoch 36: Training Loss: 4.8339\n",
      "Epoch 37: Training Loss: 4.8340\n",
      "Epoch 38: Training Loss: 4.8340\n",
      "Epoch 39: Training Loss: 4.8340\n",
      "Epoch 40: Training Loss: 4.8339\n",
      "Epoch 41: Training Loss: 4.8340\n",
      "Epoch 42: Training Loss: 4.8339\n",
      "Epoch 43: Training Loss: 4.8339\n",
      "Epoch 44: Training Loss: 4.8339\n",
      "Epoch 45: Training Loss: 4.8339\n",
      "Epoch 46: Training Loss: 4.8339\n",
      "Epoch 47: Training Loss: 4.8339\n",
      "Epoch 48: Training Loss: 4.8338\n",
      "Epoch 49: Training Loss: 4.8339\n",
      "Epoch 50: Training Loss: 4.8338\n",
      "Epoch 51: Training Loss: 4.8338\n",
      "Epoch 52: Training Loss: 4.8339\n",
      "Epoch 53: Training Loss: 4.8338\n",
      "Epoch 54: Training Loss: 4.8338\n",
      "Epoch 55: Training Loss: 4.8338\n",
      "Epoch 56: Training Loss: 4.8338\n",
      "Epoch 57: Training Loss: 4.8339\n",
      "Epoch 58: Training Loss: 4.8339\n",
      "Epoch 59: Training Loss: 4.8339\n",
      "Epoch 60: Training Loss: 4.8338\n",
      "Epoch 61: Training Loss: 4.8338\n",
      "Epoch 62: Training Loss: 4.8339\n",
      "Epoch 63: Training Loss: 4.8338\n",
      "Epoch 64: Training Loss: 4.8340\n",
      "Epoch 65: Training Loss: 4.8339\n",
      "Epoch 66: Training Loss: 4.8339\n",
      "Epoch 67: Training Loss: 4.8339\n",
      "Epoch 68: Training Loss: 4.8338\n",
      "Epoch 69: Training Loss: 4.8339\n",
      "Epoch 70: Training Loss: 4.8339\n",
      "Epoch 71: Training Loss: 4.8340\n",
      "Epoch 72: Training Loss: 4.8339\n",
      "Epoch 73: Training Loss: 4.8338\n",
      "Epoch 74: Training Loss: 4.8339\n",
      "Epoch 75: Training Loss: 4.8339\n",
      "Epoch 76: Training Loss: 4.8338\n",
      "Epoch 77: Training Loss: 4.8339\n",
      "Epoch 78: Training Loss: 4.8339\n",
      "Epoch 79: Training Loss: 4.8339\n",
      "Epoch 80: Training Loss: 4.8338\n",
      "Epoch 81: Training Loss: 4.8338\n",
      "Epoch 82: Training Loss: 4.8338\n",
      "Epoch 83: Training Loss: 4.8339\n",
      "Epoch 84: Training Loss: 4.8339\n",
      "Epoch 85: Training Loss: 4.8339\n",
      "Epoch 86: Training Loss: 4.8339\n",
      "Epoch 87: Training Loss: 4.8338\n",
      "Epoch 88: Training Loss: 4.8338\n",
      "Epoch 89: Training Loss: 4.8340\n",
      "Epoch 90: Training Loss: 4.8337\n",
      "Epoch 91: Training Loss: 4.8338\n",
      "Epoch 92: Training Loss: 4.8339\n",
      "Epoch 93: Training Loss: 4.8338\n",
      "Epoch 94: Training Loss: 4.8338\n",
      "Epoch 95: Training Loss: 4.8339\n",
      "Epoch 96: Training Loss: 4.8340\n",
      "Epoch 97: Training Loss: 4.8339\n",
      "Epoch 98: Training Loss: 4.8339\n",
      "Epoch 99: Training Loss: 4.8339\n",
      "Epoch 100: Training Loss: 4.8339\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=100, patience=50):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for waveforms, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # Ensure data has correct shape, [batch_size, 1, length]\n",
    "            waveforms = waveforms.unsqueeze(1)\n",
    "            outputs = model(waveforms)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        average_loss = total_loss / num_batches\n",
    "        print(f'Epoch {epoch+1}: Training Loss: {average_loss:.4f}')\n",
    "\n",
    "        # Early stopping logic based on training loss\n",
    "        if average_loss < best_loss:\n",
    "            best_loss = average_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # Save the best model\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model once training is finished\n",
    "    model.load_state_dict(torch.load('fine_tuned_model.pth'))\n",
    "\n",
    "train_model(model, data_loader, criterion, optimizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
